{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring\n",
    "Produce a geoJSON file containing the trueskill score associated to the perceived safety of a certain location (by analyzing pictures of that location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import trueskill\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "import cv2\n",
    "import json\n",
    "import os\n",
    "from IPython.display import display, Markdown, Latex\n",
    "import logging\n",
    "import random\n",
    "import csv\n",
    "import itertools\n",
    "\n",
    "logger = tensorflow.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n",
    "Adjust these to reflect the desired input and output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_path = \"models//CROWDSOURCING_27k_FULLDATASET_finetuned_lr-5_decreasing_Conv8MaxPool3x3B4lr0.002siameselayer1epo_then_augmentedx4lr0.000000120epochs_thenunfrozenandfinetunedagain.h5\"\n",
    "training_path = \"enhanced_preproc/\"\n",
    "data_path = \"scoring_data/\"\n",
    "csv_name = \"luzern-scoring.csv\"\n",
    "img_folder = \"Luzern/\"\n",
    "output_file = \"safety_scores/geojson_luzern_27k_exp30.json\"\n",
    "training_size = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Select which GPU to use when several are available. This can be removed when few GPUs are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pretrained ML model to compare image couples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_net = load_model(network_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select images from training set to be used as a baseline to compute trueskill score of other images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training images\n",
    "training_images = {}\n",
    "\n",
    "# Select, load and preprocess 30 random items from the training set \n",
    "for f in random.sample(os.listdir(training_path), training_size):\n",
    "    training_images[f[:-4]] = preprocess_input(cv2.imread(training_path+f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute trueskill score of all images used as training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_scores = {id : trueskill.Rating() for id in training_images}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingCouples = itertools.combinations(training_images.keys(), 2)\n",
    "\n",
    "leftIds = []\n",
    "rightIds = []\n",
    "\n",
    "for c in trainingCouples:\n",
    "    leftIds.append(c[0])\n",
    "    rightIds.append(c[1])\n",
    "\n",
    "left = np.array([training_images[l] for l in leftIds])\n",
    "right = np.array([training_images[r] for r in rightIds])\n",
    "\n",
    "result = siamese_net.predict([left, right])\n",
    "\n",
    "for j,r in enumerate(result):\n",
    "    if r[1]>r[0]: #left wins\n",
    "        newL, newR = trueskill.rate_1vs1(training_scores[leftIds[j]], training_scores[rightIds[j]])\n",
    "        training_scores[leftIds[j]] = newL\n",
    "        training_scores[rightIds[j]] = newR\n",
    "    else:\n",
    "        newR, newL = trueskill.rate_1vs1(training_scores[rightIds[j]], training_scores[leftIds[j]])\n",
    "        training_scores[leftIds[j]] = newL\n",
    "        training_scores[rightIds[j]] = newR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "Create dictionary containing images to be scored from csv file, and associated metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {}\n",
    "with open(data_path+csv_name) as f:\n",
    "    csv_data = csv.reader(f)\n",
    "    print(next(csv_data))\n",
    "    for r in csv_data:\n",
    "        metadata[r[0]] = {\"coord\": [r[2], r[3]], \"score\": trueskill.Rating()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and preprocess images to be scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = {}\n",
    "for imName in metadata.keys():\n",
    "    im = cv2.imread(data_path+img_folder+imName+\".jpg\")\n",
    "\n",
    "    if im is not None:\n",
    "        im = cv2.resize(im, (224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        images[imName] = preprocess_input(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores computation\n",
    "Compute scores for images to be analyzed by comparing them with images from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainIds = np.array(list(training_images.keys()))\n",
    "i = 0\n",
    "m = len(images.keys())\n",
    "                    \n",
    "for imgId, img in images.items():\n",
    "    i+=1\n",
    "    print(i/m*100, end = '\\r')\n",
    "    left = np.array([training_images[tId] for tId in trainIds])\n",
    "    right = np.array([img for tId in trainIds])\n",
    "\n",
    "    result = siamese_net.predict([left, right])\n",
    "\n",
    "    for j,r in enumerate(result):\n",
    "        if r[1]>r[0]: #left wins\n",
    "            _ ,metadata[imgId][\"score\"] = trueskill.rate_1vs1(training_scores[leftIds[j]], metadata[imgId][\"score\"])\n",
    "        else: #right wins\n",
    "            metadata[imgId][\"score\"], _ = trueskill.rate_1vs1(metadata[imgId][\"score\"], training_scores[leftIds[j]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only the \"mu\" value of the trueskill scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental: compare also the analyzed images\n",
    "i = 0\n",
    "m = len(images.keys())\n",
    "                    \n",
    "for imgId, img in images.items():\n",
    "    i+=1\n",
    "    print(i/m*100, end = '\\r')\n",
    "    leftIds = [tId for tId in random.sample(images.keys(),30)]\n",
    "    left = np.array([images[tId] for tId in leftIds])\n",
    "    right = np.array([img for tId in range(0,30)])\n",
    "\n",
    "    result = siamese_net.predict([left, right])\n",
    "\n",
    "    for j,r in enumerate(result):\n",
    "        if r[1]>r[0]: #left wins\n",
    "            metadata[leftIds[j]][\"score\"] ,metadata[imgId][\"score\"] = trueskill.rate_1vs1(metadata[leftIds[j]][\"score\"], metadata[imgId][\"score\"])\n",
    "        else: #right wins\n",
    "            metadata[imgId][\"score\"], metadata[leftIds[j]][\"score\"] = trueskill.rate_1vs1(metadata[imgId][\"score\"], metadata[leftIds[j]][\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "mink = list(metadata.keys())[0]\n",
    "maxk = list(metadata.keys())[0]\n",
    "\n",
    "for k in metadata.keys():\n",
    "    metadata[k][\"score\"] = metadata[k][\"score\"].mu\n",
    "    \n",
    "    if metadata[k][\"score\"] > metadata[maxk][\"score\"]:\n",
    "        maxk = k\n",
    "    if metadata[k][\"score\"] < metadata[mink][\"score\"]:\n",
    "        mink = k  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(im1, im2, label):\n",
    "    im1 = im1+127.5\n",
    "    im1[im1>255] = 255\n",
    "    im1 = cv2.cvtColor(im1.astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "    im2 = im2+127.5\n",
    "    im2[im2>255] = 255\n",
    "    im2 = cv2.cvtColor(im2.astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize = (15,10))\n",
    "    ax1.imshow(im1)\n",
    "    ax1.axis('off')\n",
    "    ax2.imshow(im2)\n",
    "    ax2.axis('off')\n",
    "    plt.title(label)\n",
    "    plt.show()\n",
    "\n",
    "display_images(images[mink], images[maxk], \"min, max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform data to geoJSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geojson = {\"type\": \"FeatureCollection\",\n",
    "  \"features\":[]}\n",
    "\n",
    "for k, v in metadata.items():\n",
    "    geojson[\"features\"].append({\"type\": \"Feature\",\n",
    "                   \"properties\": {\n",
    "                       \"name\": k,\n",
    "                       \"score\": v[\"score\"]\n",
    "                   },\n",
    "                   \"geometry\": {\n",
    "                       \"type\": \"Point\",\n",
    "                       \"coordinates\": [float(v[\"coord\"][1]), float(v[\"coord\"][0])]\n",
    "                   }})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save geoJSON data to external json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, 'w') as fp:\n",
    "    json.dump(geojson, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
